---
title: 'Exercise 5: Dimensionality reduction techniques'
author: "Harri Lindroos"
date: "2 joulukuuta 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#RStudio exercise 5: Dimensionality reduction techniques

#Reading the data

This weeks data is provided by United Nations Development Programme, and it consists of different variables concerning human development. Our prepared dataset is a partial combination of two indices: The Human Development Index (HDI) and The Gender Inequality Index. As we can see from below, the dataset consists of 155 countries (observations) and 8 variables: 

```{r}
hdgii <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human2.txt")
dim(hdgii)
```

The variables measure per capita gross national income (GNI), life expectancy at birth (Life.Exp), expected years of schooling (Edu.Exp), maternal mortality ratio (Mat.Mor), adolescent birth rate (Ado.Birth), Percetange of female representatives in parliament (Parli.F), the ratio of females with at least secondary education compared to men with secondary education (Edu2.FM), and the ratio of female and men populations in the labour force (Labo.FM). The values and ranges of all the variables can be observed below:

```{r}
summary(hdgii)
```

#Correlations

Next we are going to take a look at the correlations among our data. One simple and elightening way to examine this is to calculate a correlations matrix and to plot it. Our correlations plot colours negative correlations (0>x>-1) with different shades of red and positive correlations (-1>x>0) with shades of blue. Higher correlation values (the bigegr the distance is from the zero) are also presented with bigger dots:

```{r}
library(dplyr)
library(corrplot)
library(GGally)
cor(hdgii) %>% corrplot(method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)
```

Judging by the above plot the two stand out correlations are the positive correlation for life expectancy at birth and expected years of schooling, and the negative one for maternal mortality ratio and life expectancy at birth. In my opinion these correlations fit very well with common sense presumptions about human development: When . It is also possible to examine these correlations in more precise manner by plotting the same correlations matrix with numerical values. This matrix plot confirms our previous assumptions concerning the strongest correlations: The exact numerical value for the correlation between life expectancy at birth and expected years of schooling is 0.789 and the correlation for maternal mortality ratio and life expectancy at birth is -0.857. The later is the highest correlation in our data. The lowest correlations are the values of ~ 0.0096 for "Labo.FM"" and "Edu2.FM"" and ~ -0.0071 for "Ado.Birth"" and "Parli.F". These correlations are very small and the former one is nearly non-existant. All the correlations are presented in the matrix below:

```{r}
ggpairs(hdgii)
```

##PCA

###PCA with unscaled variables

One way to analyse correlations in a given dataset is to limit its dimensions. This is useful when we don't divide variables into target and explanatory variables. This means that we have to take into account all the dimensions and relationships in the dataset. And the bigger the dataset, the more messier and complex the full (especially visual) analysis. This is why it's useful to resort to dimensionality reduction techniques such as principal component analysis (PCA). With PCA we can reduce any number of measured and correlated variables into a few independent components to account for the variation in the data. Let's try this:

```{r}
pca_hdgii <- prcomp(hdgii)
summary(pca_hdgii)
```

The above summary shows the results of PCA on our data. As we can see, our analysis gives us 8 components (PC1 to PC8), to match our 8 variables, to account for the variance. With PCA we can rank these components to account for the variance with only few components: This is the reduction part of our analysis. The thing that makes this kind of move effective is that the explanatory power of the component reduces with every added component. This can be seen form our summarys cumulative proportion -row, which reaches 1.00 (100%) with two components. But this is only relatively interesting since the proportion of variance of our PCA is very, very small: Our component collect together only the hundreth part of percent of the total variance, with the second component (PC2) doing most of the work. Something must be wrong here. The biplot with two most effective components has a lot to tell: 

```{r}
biplot(pca_hdgii, choices = 1:2, cex = c(0.8, 1), col = c("grey40", "deeppink2"))
```

Our PCA plot looks just as uninformative and cryptic as the summary suggested. The reason for this can be found from our datasets original summary. There we can see the large difference between mediums, ranges and values of our variables. The single largest value in our set is as big as 123124, which is the GNI score of Qatar. Compared to this the values of, for instance, "Labo.FM" and "Edu2.FM" are very small (both maximums are around 1.00). The immense value of the standard deviation of our PC2 (185.5219, see the summary) tells everything we need to know As we can see in the left hand side of our plot, there is big gap between any other country and Qatar, and the first component accounts only for this difference in GNI scores. The length and direction of the GNI vector tells the same thing. Most of our observations and vectors are grouped together and are imposible to observe. Only few outliers can be detected mainly because of their high GNI values and its effect on PC1, or because of high maternal mortality rates (Sierra Leone!). To get better and more fruitful results, we have to standardize our data. 

###Standardized

Let's star with the summary of our new, tsandardized version of our old dataset. As we can see, the numbers look very different compared to our first summary: The mean values are set (scaled) to zero (0.0000) and the values are much closer to each other. But the main thing stays the same: The numbers account for the variance in our variables. The numbers can be seen from the summary below:  

```{r}
hdgii_scaled <- scale(hdgii)
summary(hdgii_scaled)
```

Now when the variables are scaled, we can present them using PCA. First the summary of the model:

```{r}
pca_scaled <- prcomp(hdgii_scaled)
summary(pca_scaled)
```

As we can see, with standardized data our numbers look very different. The standard deviation values are smaller and, most importantly, our components account for the overall variance in our data: Pc1 is responsible of circa 54 percent, and PC2 circa 16 percent. Together they account for nearly 70 percent of the complete variance. This is how our plot matrix looks like: 

```{r}
biplot(pca_scaled, choices = 1:2, cex = c(0.8, 1), col = c("grey40", "deeppink2"))
```

Compared to the first biplot this plot really helps us to analyse our dataset. The vectors are all easy to separate, and their correlations (closeness directions and angles) are easy to see. We can, for example, see that the relative proportion of females in the labour force (Labo.FM) correlates more with female representation in parliament (Parli.F) than with any other variable. The secon component (CP2) accounts mainly for variance in these variables. The PC1 (x-axis) accounts for all the other variables. The spread of this axis shows clearly the high positive correlation between maternal mortality ratio (Mat.Mor) and adolescent birth rate (Ado.Birth). The same principles of spatial closeness applys also for our observations (countries in this case). Once again the outliers are the easiest to spot: Rwanda with its unique history has high values on y-axis (PC2) and on this axis it's the furthest away from countries such as Iran. Then again on x-axis (PC2) Iran is nearly at the opposite end compared to Niger, which has the lowest overall points in our dataset. 

##Multiple Correspondence Analysis (MCA) with the tea dataset

```{r}
library(FactoMineR)
data("tea")
str(tea)
```

```{r}
keep_columns <- c("Tea", "How", "how", "sugar", "where", "lunch")

tea_time <- select(tea, one_of(keep_columns))


summary(tea_time)
str(tea_time)

```

```{r}

```

